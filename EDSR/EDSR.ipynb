{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "EDSR.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x8ZY_mJThJad",
        "colab_type": "code",
        "outputId": "ca4c41c9-e5ce-4042-f3ef-1ea7d5dc758b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "import random\n",
        "import glob\n",
        "import subprocess\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow.keras as Ke\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "# from tensorflow.keras.layers import BatchNormalization as BN\n",
        "# from tensorflow.keras.layers import Input, Conv2D, BatchNormalization, Activation, Dense, Add, UpSampling2D, Concatenate, Layer, Dropout, MaxPool2D, Cropping2D,Lambda\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, Callback\n",
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras import *\n",
        "import wandb\n",
        "from wandb.keras import WandbCallback\n",
        "import cv2\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.models import Model"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tn2PaWPChs6r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_dir = 'data/test'\n",
        "train_dir = 'data/train'\n",
        "\n",
        "# automatically get the data if it doesn't exist\n",
        "if not os.path.exists(\"data\"):\n",
        "    print(\"Downloading flower dataset...\")\n",
        "    subprocess.check_output(\n",
        "        \"mkdir data && curl https://storage.googleapis.com/wandb/flower-enhance.tar.gz | tar xz -C data\", shell=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiZpBwzTh61f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "dest_path='data/train'\n",
        "\n",
        "def rotate(image, file_name,rotate_deg):\n",
        "    rotate_90 = image.rotate(rotate_deg)\n",
        "#     print(file_name.split('/'))\n",
        "    rotate_90.save(os.path.join(dest_path,'R9'+file_name.split('/')[2]))\n",
        "\n",
        "def mirror(image,file_name):\n",
        "    H_img = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    V_img = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "\n",
        "    V_img.save(os.path.join(dest_path,'VF'+file_name.split('/')[2]))\n",
        "    H_img.save(os.path.join(dest_path,'HF'+file_name.split('/')[2]))\n",
        "\n",
        "def image_auggenerator( img_dir):\n",
        "    \"\"\"A generator that returns small images and large images.  DO NOT ALTER the validation set\"\"\"\n",
        "    input_filenames = glob.glob(img_dir + \"/*-in.jpg\")\n",
        "    counter = 0\n",
        "    for i in range (0,len(input_filenames)):\n",
        "        img=input_filenames[counter]\n",
        "        in_image=Image.open(img)#cv2.imread(img)\n",
        "        rotate(in_image,img,90)\n",
        "        mirror(in_image,img)\n",
        "        out_image=Image.open(img.replace(\"-in.jpg\", \"-out.jpg\"))#cv2.imread(img.replace(\"-in.jpg\", \"-out.jpg\"))\n",
        "        rotate(out_image,img.replace(\"-in.jpg\", \"-out.jpg\"),90)\n",
        "        mirror(out_image,img.replace(\"-in.jpg\", \"-out.jpg\"))\n",
        "        counter+=1\n",
        "\n",
        "img_dir='data/train'\n",
        "\n",
        "image_auggenerator(img_dir)\n",
        "print(len(os.listdir('data/train')))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CO6PzN6qhFm1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "config = run.config\n",
        "config.num_epochs = 10\n",
        "config.batch_size = 4\n",
        "config.input_height = 64\n",
        "config.input_width = 64\n",
        "config.output_height = 256\n",
        "config.output_width = 256\n",
        "\n",
        "\n",
        "config.steps_per_epoch = len(\n",
        "    glob.glob(train_dir + \"/*-in.jpg\")) // config.batch_size\n",
        "config.val_steps_per_epoch = len(\n",
        "    glob.glob(val_dir + \"/*-in.jpg\")) // config.batch_size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def image_generator(batch_size, img_dir):\n",
        "    \"\"\"A generator that returns small images and large images.  DO NOT ALTER the validation set\"\"\"\n",
        "    input_filenames = glob.glob(img_dir + \"/*-in.jpg\")\n",
        "    counter = 0\n",
        "    while True:\n",
        "        small_images = np.zeros(\n",
        "            (batch_size, config.input_width, config.input_height, 3))\n",
        "        large_images = np.zeros(\n",
        "            (batch_size, config.output_width, config.output_height, 3))\n",
        "        random.shuffle(input_filenames)\n",
        "        if counter+batch_size >= len(input_filenames):\n",
        "            counter = 0\n",
        "        for i in range(batch_size):\n",
        "            img = input_filenames[counter + i]\n",
        "            in_img=cv2.imread((img))\n",
        "            in_img=cv2.resize(in_img,(64,64))\n",
        "            small_images[i] = np.array(in_img) / 255.0\n",
        "            large_images[i] = np.array(\n",
        "                cv2.imread(img.replace(\"-in.jpg\", \"-out.jpg\"))) / 255.0\n",
        "        yield (small_images, large_images)\n",
        "        counter += batch_size\n",
        "\n",
        "\n",
        "def perceptual_distance(y_true, y_pred):\n",
        "    \"\"\"Calculate perceptual distance, DO NOT ALTER\"\"\"\n",
        "    y_true *= 255\n",
        "    y_pred *= 255\n",
        "    rmean = (y_true[:, :, :, 0] + y_pred[:, :, :, 0]) / 2\n",
        "    r = y_true[:, :, :, 0] - y_pred[:, :, :, 0]\n",
        "    g = y_true[:, :, :, 1] - y_pred[:, :, :, 1]\n",
        "    b = y_true[:, :, :, 2] - y_pred[:, :, :, 2]\n",
        "\n",
        "    return K.mean(K.sqrt((((512+rmean)*r*r)/256) + 4*g*g + (((767-rmean)*b*b)/256)))\n",
        "\n",
        "\n",
        "val_generator = image_generator(config.batch_size, val_dir)\n",
        "in_sample_images, out_sample_images = next(val_generator)\n",
        "\n",
        "\n",
        "class ImageLogger(Callback):\n",
        "    def on_epoch_end(self, epoch, logs):\n",
        "        preds = self.model.predict(in_sample_images)\n",
        "        in_resized = []\n",
        "        for arr in in_sample_images:\n",
        "            # Simple upsampling\n",
        "            in_resized.append(arr.repeat(8, axis=0).repeat(8, axis=1))\n",
        "        wandb.log({\n",
        "            \"examples\": [wandb.Image(np.concatenate([in_resized[i] * 255, o * 255, out_sample_images[i] * 255], axis=1)) for i, o in enumerate(preds)]\n",
        "        }, commit=False)\n",
        "\n",
        "        \n",
        "\n",
        "        \n",
        "        \n",
        "def ms_ssmi_loss(y_true,y_pred):\n",
        "  ssim_loss=K.mean(tf.image.ssim_multiscale(y_true,y_pred,1.0))\n",
        "  l1_loss=K.sum(tf.keras.losses.mean_absolute_error(y_true, y_pred),axis=1)\n",
        "  loss = 1.5*ssim_loss+l1_loss*(0.5)\n",
        "  return loss\n",
        "  \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ypw5yDkjgse-",
        "colab_type": "code",
        "outputId": "3e7790f5-fd6a-4f24-a151-2d09d4373c99",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        }
      },
      "source": [
        "import numpy as np\n",
        "import glob\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import os\n",
        "\n",
        "dest_path='data/train'\n",
        "\n",
        "def rotate(image, file_name,rotate_deg):\n",
        "    rotate_90 = image.rotate(rotate_deg)\n",
        "#     print(file_name.split('/'))\n",
        "    rotate_90.save(os.path.join(dest_path,'R9'+file_name.split('/')[2]))\n",
        "\n",
        "def mirror(image,file_name):\n",
        "    H_img = image.transpose(Image.FLIP_LEFT_RIGHT)\n",
        "    V_img = image.transpose(Image.FLIP_TOP_BOTTOM)\n",
        "\n",
        "    V_img.save(os.path.join(dest_path,'VF'+file_name.split('/')[2]))\n",
        "    H_img.save(os.path.join(dest_path,'HF'+file_name.split('/')[2]))\n",
        "\n",
        "def image_auggenerator( img_dir):\n",
        "    \"\"\"A generator that returns small images and large images.  DO NOT ALTER the validation set\"\"\"\n",
        "    input_filenames = glob.glob(img_dir + \"/*-in.jpg\")\n",
        "    counter = 0\n",
        "    for i in range (0,len(input_filenames)):\n",
        "        img=input_filenames[counter]\n",
        "        in_image=Image.open(img)#cv2.imread(img)\n",
        "        rotate(in_image,img,90)\n",
        "        mirror(in_image,img)\n",
        "        out_image=Image.open(img.replace(\"-in.jpg\", \"-out.jpg\"))#cv2.imread(img.replace(\"-in.jpg\", \"-out.jpg\"))\n",
        "        rotate(out_image,img.replace(\"-in.jpg\", \"-out.jpg\"),90)\n",
        "        mirror(out_image,img.replace(\"-in.jpg\", \"-out.jpg\"))\n",
        "        counter+=1\n",
        "\n",
        "img_dir='data/train'\n",
        "\n",
        "image_auggenerator(img_dir)\n",
        "print(len(os.listdir('data/train')))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Host key verification failed.\r\n",
            "fatal: Could not read from remote repository.\n",
            "\n",
            "Please make sure you have the correct access rights\n",
            "and the repository exists.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vQ8PE-0HAMW6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.models import Model\n",
        "import numpy as np\n",
        "import h5py\n",
        "\n",
        "# base_model = VGG19(weights='imagenet')\n",
        "# print(base_model.summary())\n",
        "# model = Model(inputs=base_model.input, outputs=base_model.get_layer('relu3_3').output)\n",
        "#\n",
        "# img_path = r'C:\\Users\\vghorpad\\OneDrive - Qualcomm\\Pictures\\medium_exposure.jpg'\n",
        "# img = image.load_img(img_path, target_size=(224, 224))\n",
        "# x = image.img_to_array(img)\n",
        "# x = np.expand_dims(x, axis=0)\n",
        "# x = preprocess_input(x)\n",
        "#\n",
        "# block4_pool_features = model.predict(x)\n",
        "import random\n",
        "import glob\n",
        "import subprocess\n",
        "import os\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import backend as K\n",
        "import tensorflow.keras as Ke\n",
        "from tensorflow.keras.callbacks import Callback\n",
        "from tensorflow.keras.layers import BatchNormalization as BN\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "# import wandb\n",
        "from tensorflow.keras import *\n",
        "from tensorflow.keras.layers import *\n",
        "from wandb.keras import WandbCallback\n",
        "import cv2\n",
        "from tensorflow.keras.applications.vgg19 import VGG19\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg19 import preprocess_input\n",
        "from tensorflow.keras.models import Model,load_model\n",
        "\n",
        "\n",
        "def icnr_weights(init=tf.glorot_normal_initializer(), scale=2, shape=[3, 3, 32, 4], dtype=tf.float32):\n",
        "    sess = tf.Session()\n",
        "    return sess.run(ICNR(init, scale=scale)(shape=shape, dtype=dtype))\n",
        "\n",
        "\n",
        "class ICNR:\n",
        "    \"\"\"ICNR initializer for checkerboard artifact free sub pixel convolution\n",
        "    Ref:\n",
        "     [1] Andrew Aitken et al. Checkerboard artifact free sub-pixel convolution\n",
        "     https://arxiv.org/pdf/1707.02937.pdf)\n",
        "    Args:\n",
        "    initializer: initializer used for sub kernels (orthogonal, glorot uniform, etc.)\n",
        "    scale: scale factor of sub pixel convolution\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, initializer, scale=1):\n",
        "        self.scale = scale\n",
        "        self.initializer = initializer\n",
        "\n",
        "    def __call__(self, shape, dtype, partition_info=None):\n",
        "        shape = list(shape)\n",
        "        if self.scale == 1:\n",
        "            return self.initializer(shape)\n",
        "\n",
        "        new_shape = shape[:3] + [shape[3] // (self.scale ** 2)]\n",
        "        x = self.initializer(new_shape, dtype, partition_info)\n",
        "        x = tf.transpose(x, perm=[2, 0, 1, 3])\n",
        "        x = tf.image.resize_nearest_neighbor(x, size=(shape[0] * self.scale, shape[1] * self.scale))\n",
        "        x = tf.space_to_depth(x, block_size=self.scale)\n",
        "        x = tf.transpose(x, perm=[1, 2, 0, 3])\n",
        "\n",
        "        return x\n",
        "data_format='channels_first'\n",
        "\n",
        "class Subpixel(Conv2D):\n",
        "    def __init__(self,\n",
        "                 filters,\n",
        "                 kernel_size,\n",
        "                 r,\n",
        "                 padding='valid',\n",
        "                 data_format=data_format,\n",
        "                 strides=(1, 1),\n",
        "                 activation=None,\n",
        "                 use_bias=True,\n",
        "                 kernel_initializer='glorot_uniform',\n",
        "                 bias_initializer='zeros',\n",
        "                 kernel_regularizer=None,\n",
        "                 bias_regularizer=None,\n",
        "                 activity_regularizer=None,\n",
        "                 kernel_constraint=None,\n",
        "                 bias_constraint=None,\n",
        "                 **kwargs):\n",
        "        super(Subpixel, self).__init__(\n",
        "            filters=r * r * filters,\n",
        "            kernel_size=kernel_size,\n",
        "            strides=strides,\n",
        "            padding=padding,\n",
        "            data_format=data_format,\n",
        "            activation=activation,\n",
        "            use_bias=use_bias,\n",
        "            kernel_initializer=kernel_initializer,\n",
        "            bias_initializer=bias_initializer,\n",
        "            kernel_regularizer=kernel_regularizer,\n",
        "            bias_regularizer=bias_regularizer,\n",
        "            activity_regularizer=activity_regularizer,\n",
        "            kernel_constraint=kernel_constraint,\n",
        "            bias_constraint=bias_constraint,\n",
        "            **kwargs)\n",
        "        self.r = r\n",
        "\n",
        "    def _phase_shift(self, I):\n",
        "        r = self.r\n",
        "        bsize, a, b, c = I.get_shape().as_list()\n",
        "        bsize = K.shape(I)[0]  # Handling Dimension(None) type for undefined batch dim\n",
        "        X = K.reshape(I, [bsize, a, b, int(c / (r * r)), r, r])  # bsize, a, b, c/(r*r), r, r\n",
        "        X = K.permute_dimensions(X, (0, 1, 2, 5, 4, 3))  # bsize, a, b, r, r, c/(r*r)\n",
        "        # Keras backend does not support tf.split, so in future versions this could be nicer\n",
        "        X = [X[:, i, :, :, :, :] for i in range(a)]  # a, [bsize, b, r, r, c/(r*r)\n",
        "        X = K.concatenate(X, 2)  # bsize, b, a*r, r, c/(r*r)\n",
        "        X = [X[:, i, :, :, :] for i in range(b)]  # b, [bsize, r, r, c/(r*r)\n",
        "        X = K.concatenate(X, 2)  # bsize, a*r, b*r, c/(r*r)\n",
        "        return X\n",
        "\n",
        "    def call(self, inputs):\n",
        "        return self._phase_shift(super(Subpixel, self).call(inputs))\n",
        "\n",
        "    def compute_output_shape(self, input_shape):\n",
        "        unshifted = super(Subpixel, self).compute_output_shape(input_shape)\n",
        "        return (unshifted[0], self.r * unshifted[1], self.r * unshifted[2], int(unshifted[3] / (self.r * self.r)))\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(Conv2D, self).get_config()\n",
        "        config.pop('rank')\n",
        "        config.pop('dilation_rate')\n",
        "        config['filters'] = int(config['filters'] / self.r * self.r)\n",
        "        config['r'] = self.r\n",
        "        return config\n",
        "\n",
        "scale=4\n",
        "def EDSR(scale=4, input_shape=(64, 64, 3), n_feats=64, n_resblocks=16):\n",
        "    '''\n",
        "        According to the paper scale can be 2,3 or 4.\n",
        "        However this code supports scale to be 3 or any of 2^n for n>0\n",
        "    '''\n",
        "\n",
        "    def res_block(input_tensor, nf, res_scale=1.0):\n",
        "        x = Conv2D(nf, (3, 3), padding='same', activation='relu',\n",
        "                   activity_regularizer=regularizers.l1(10e-10))(input_tensor)\n",
        "        x = Conv2D(nf, (3, 3), padding='same', activity_regularizer=regularizers.l1(10e-10))(x)\n",
        "        x = Lambda(lambda x: x * res_scale)(x)\n",
        "        print(x.shape)\n",
        "        x = add([x, input_tensor])\n",
        "        return x\n",
        "\n",
        "    inp = Input(shape=input_shape)\n",
        "\n",
        "    x = Conv2D(n_feats, 3, padding='same', activity_regularizer=regularizers.l1(10e-10),data_format=data_format)(inp)\n",
        "    conv1 = x\n",
        "    if n_feats == 256:\n",
        "        res_scale = 0.1\n",
        "    else:\n",
        "        res_scale = 1.0\n",
        "    for i in range(n_resblocks): x = res_block(x, n_feats, res_scale)\n",
        "    x = Conv2D(n_feats, 3, padding='same', activity_regularizer=regularizers.l1(10e-10),data_format=data_format)(x)\n",
        "    x = add([x, conv1])\n",
        "\n",
        "    if not scale % 2:\n",
        "        for i in range(int(np.log2(scale))):\n",
        "            x = Subpixel(n_feats, 3, 2, padding='same', activity_regularizer=regularizers.l1(10e-10),data_format=data_format)(x)\n",
        "    else:  # scale = 3\n",
        "        x = Subpixel(n_feats, 3, 3, padding='same', activity_regularizer=regularizers.l1(10e-10),data_format=data_format)(x)\n",
        "    sr = Conv2D(input_shape[-1], 1, padding='same',\n",
        "                activity_regularizer=regularizers.l1(10e-10),data_format=data_format)(x)\n",
        "\n",
        "    model = Model(inputs=inp, outputs=sr, name='SR')\n",
        "    \"\"\"ICNR initializer for checkerboard artifact free sub pixel convolution\n",
        "        Ref:\n",
        "         [1] Andrew Aitken et al. Checkerboard artifact free sub-pixel convolution\n",
        "         https://arxiv.org/pdf/1707.02937.pdf)\n",
        "    \"\"\"\n",
        "    for layer in model.layers:\n",
        "        if type(layer) == Subpixel:\n",
        "            c, b = layer.get_weights()\n",
        "            if scale == 3:\n",
        "                w = icnr_weights(scale=3, shape=c.shape, dtype = tf.float32)\n",
        "            else:\n",
        "                w = icnr_weights(scale=2, shape=c.shape, dtype = tf.float32)\n",
        "            layer.set_weights([w, b])\n",
        "    return model\n",
        "\n",
        "\n",
        "model = EDSR()\n",
        "model.summary()\n",
        "model.load_weights('edsr-16-x4-gen-pre-psnr-28.8885.h5')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TAsVG9JAeSb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer=Adam(lr=0.005), loss='mae', metrics=[perceptual_distance])\n",
        "\n",
        "model.fit_generator(image_generator(config.batch_size, train_dir),\n",
        "                    steps_per_epoch=config.steps_per_epoch,\n",
        "                    epochs=config.num_epochs, callbacks=[\n",
        "                        ImageLogger(), WandbCallback()],\n",
        "                    validation_steps=config.val_steps_per_epoch,\n",
        "                    validation_data=val_generator)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}